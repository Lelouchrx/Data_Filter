import cv2
import mediapipe as mp
import sys
import time
import numpy as np
import os
# ã€æ–°å¢ã€‘å¯¼å…¥ YOLO
from ultralytics import YOLO

class HandDataFilter:
    def __init__(self, min_conf=0.5, missing_tolerance=10, check_border=True):
        self.mp_hands = mp.solutions.hands
        # æé«˜é˜ˆå€¼åˆ° 0.7ï¼Œä¿è¯æ•°æ®è´¨é‡
        self.hands = self.mp_hands.Hands(
            static_image_mode=False,
            max_num_hands=2,
            min_detection_confidence=min_conf,
            min_tracking_confidence=min_conf
        )
        self.tolerance = missing_tolerance # å…è®¸è¿ç»­ä¸¢å¤±çš„æœ€å¤§å¸§æ•°
        self.check_border = check_border   # æ˜¯å¦å¼€å¯è¾¹ç¼˜æ£€æŸ¥
        self.margin = 0.05                 # è¾¹ç¼˜ç•™ç™½ 5%
        self.mp_drawing = mp.solutions.drawing_utils

        # ã€æ–°å¢ã€‘åŠ è½½ YOLOv8 Small æ¨¡å‹
        # åŸä»£ç ï¼š
        # self.yolo_model = YOLO('yolov8n.pt') 
        
        # ä¿®æ”¹åï¼šæ¢æˆ yolov8s.pt (å°) æˆ– yolov8m.pt (ä¸­)
        # ç¬¬ä¸€æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½ï¼Œå¤§æ¦‚ 20MB - 50MB
        print("æ­£åœ¨åŠ è½½ YOLOv8 Small æ¨¡å‹...")
        self.yolo_model = YOLO('yolov8s.pt') 
        # è®¾ç½®ä¸æƒ³æ£€æµ‹çš„ç±»åˆ« (æ¯”å¦‚æŠŠ'äºº'å±è”½æ‰ï¼Œåªæ£€æµ‹ç‰©ä½“)
        # COCOæ•°æ®é›†: 0 is person. æˆ‘ä»¬åªéœ€è¦ç‰©ä½“ã€‚
        self.ignored_classes = [0]

    # --- ã€æ–°å¢ã€‘äº¤äº’çŠ¶æ€åˆ†æå‡½æ•° ---
    def analyze_interaction_state(self, hand_landmarks):
        """
        åˆ¤æ–­æ‰‹éƒ¨çŠ¶æ€: Open (å¼ å¼€), Fist (ç©ºæ‹³), Grasping (æŠ“å–)
        """
        # 1. è·å–å…³é”®ç‚¹åæ ‡ (x, y)
        # æ‹‡æŒ‡(4), é£ŸæŒ‡(8), ä¸­æŒ‡(12), æ— åæŒ‡(16), å°æŒ‡(20), æ‰‹è…•(0)
        points = {}
        for idx in [0, 4, 8, 12, 16, 20]:
            points[idx] = np.array([hand_landmarks.landmark[idx].x, hand_landmarks.landmark[idx].y])
        
        # 2. è®¡ç®— æ‹‡æŒ‡-é£ŸæŒ‡ è·ç¦» (Pinch Check)
        pinch_dist = np.linalg.norm(points[4] - points[8])
        is_pinching = pinch_dist < 0.08  # é˜ˆå€¼å¯å¾®è°ƒ
        
        if not is_pinching:
            return "Open", (0, 255, 0) # ç»¿è‰²
        
        # 3. è®¡ç®— æŒå¿ƒæ‹¥æŒ¤åº¦ (æŒ‡å°–åˆ°æ‰‹è…•çš„è·ç¦»)
        # ç”¨ ä¸­æŒ‡ã€æ— åæŒ‡ã€å°æŒ‡ åˆ° æ‰‹è…•çš„å¹³å‡è·ç¦»
        finger_to_wrist_dists = [
            np.linalg.norm(points[i] - points[0]) for i in [12, 16, 20]
        ]
        avg_curl_dist = np.mean(finger_to_wrist_dists)
        
        # 4. åŒºåˆ† ç©ºæ‹³ vs æŠ“å–
        # ç»éªŒé˜ˆå€¼ï¼š< 0.25 è¯´æ˜æ‰‹æŒ‡ç¼©å¾—å¾ˆç´§ï¼ˆç©ºæ‹³ï¼‰ï¼Œ> 0.25 è¯´æ˜æœ‰ä½“ç§¯æ’‘ç€ï¼ˆæŠ“å–ï¼‰
        if avg_curl_dist < 0.25: 
            return "Fist", (0, 255, 255)    # é»„è‰² (ç©ºæ¡)
        else:
            return "Grasping", (0, 0, 255)  # çº¢è‰² (æŠ“å–ä¸­)

    # --- ã€æ–°å¢ã€‘è·å–æ‰‹çš„è¾¹ç•Œæ¡† (Bounding Box) ---
    def get_hand_bbox(self, landmarks, frame_w, frame_h):
        """å°†å½’ä¸€åŒ–çš„å…³é”®ç‚¹è½¬æ¢ä¸ºåƒç´ åæ ‡çš„è¾¹ç•Œæ¡† [x1, y1, x2, y2]"""
        x_list = [lm.x for lm in landmarks.landmark]
        y_list = [lm.y for lm in landmarks.landmark]
        
        x1, x2 = min(x_list), max(x_list)
        y1, y2 = min(y_list), max(y_list)
        
        # ç¨å¾®ç»™æ‰‹éƒ¨æ¡†åŠ ä¸€ç‚¹ padding (æ‰©å…… 10%)ï¼Œè®©é‡å æ£€æµ‹æ›´çµæ•
        padding = 0.05
        x1 = max(0, x1 - padding)
        y1 = max(0, y1 - padding)
        x2 = min(1, x2 + padding)
        y2 = min(1, y2 + padding)

        return [int(x1 * frame_w), int(y1 * frame_h), int(x2 * frame_w), int(y2 * frame_h)]

    # --- ã€æ–°å¢ã€‘è®¡ç®— IoU é‡å å¹¶åˆ¤æ–­äº¤äº’ ---
    def check_interaction_with_yolo(self, frame, hand_bbox):
        """
        è¿è¡Œ YOLOï¼Œçœ‹æ‰‹éƒ¨æ¡†æ˜¯å¦ä¸ä»»ä½•ç‰©ä½“æ¡†é‡å 
        è¿”å›: (æ˜¯å¦äº¤äº’, ç‰©ä½“åç§°, ç‰©ä½“æ¡†)
        """
        # å»ºè®®åœ¨è¿™é‡Œä¹Ÿè½¬ä¸€ä¸‹ï¼Œç¡®ä¿ YOLO æ‹¿åˆ°çš„æ˜¯æ ‡å‡†æ ¼å¼
        # å¦‚æœä¼ å…¥çš„æ˜¯ RGB (MediaPipeçš„frame)ï¼ŒYOLO èƒ½å¤„ç†
        # å¦‚æœä¼ å…¥çš„æ˜¯ BGR (OpenCVçš„frame)ï¼ŒYOLO ä¹Ÿèƒ½å¤„ç†
        # å…³é”®æ˜¯ï¼šä¸è¦ä¼  flag.writeable=False çš„åªè¯»å†…å­˜è¿›å»
        
        # è¿è¡Œæ¨ç†
        # åŸä»£ç ï¼š
        # results = self.yolo_model(frame, verbose=False, conf=0.3)
        
        # ä¿®æ”¹åï¼šé™ä½åˆ° 0.15ï¼Œåªè¦æœ‰ä¸€ç‚¹åƒæ¯å­å°±è®¤
        # ç°åœ¨çš„ conf=0.3 å¯èƒ½å¤ªé«˜äº†ã€‚åœ¨é®æŒ¡æƒ…å†µä¸‹ï¼ŒYOLO å¯¹è¿™ä¸ªæ¯å­çš„ç½®ä¿¡åº¦å¯èƒ½åªæœ‰ 0.15 å·¦å³ã€‚
        results = self.yolo_model(frame, verbose=False, conf=0.15)
        
        detected_objects = results[0].boxes
        
        hx1, hy1, hx2, hy2 = hand_bbox
        hand_area = (hx2 - hx1) * (hy2 - hy1)

        best_iou = 0
        interaction_obj = None
        obj_box_coords = None

        for box in detected_objects:
            cls_id = int(box.cls[0])
            
            # è¿‡æ»¤æ‰ "äºº" (class 0)ï¼Œæˆ‘ä»¬ä¸æŠŠè‡ªå·±çš„èº«ä½“å½“ç‰©ä½“
            if cls_id in self.ignored_classes:
                continue

            # è·å–ç‰©ä½“åæ ‡
            ox1, oy1, ox2, oy2 = map(int, box.xyxy[0])
            obj_name = self.yolo_model.names[cls_id]

            # === è®¡ç®—é‡å é¢ç§¯ (Intersection) ===
            ix1 = max(hx1, ox1)
            iy1 = max(hy1, oy1)
            ix2 = min(hx2, ox2)
            iy2 = min(hy2, oy2)

            inter_width = max(0, ix2 - ix1)
            inter_height = max(0, iy2 - iy1)
            inter_area = inter_width * inter_height

            # å¦‚æœé‡å é¢ç§¯ > 0ï¼Œè¯´æ˜ç¢°åˆ°äº†
            if inter_area > 0:
                # è¿™æ˜¯ä¸€ä¸ªç®€å•çš„ IoU å˜ä½“ï¼Œæˆ‘ä»¬çœ‹é‡å éƒ¨åˆ†å æ‰‹éƒ¨é¢ç§¯çš„æ¯”ä¾‹
                # æˆ–è€…åªè¦æœ‰é‡å å°±ç®—äº¤äº’
                interaction_obj = obj_name
                obj_box_coords = (ox1, oy1, ox2, oy2)
                return True, obj_name, obj_box_coords

        return False, None, None

    def is_hand_valid(self, hand_landmarks):
        """
        æ£€æŸ¥å•åªæ‰‹æ˜¯å¦æœ‰æ•ˆï¼š
        1. å­˜åœ¨
        2. æ‰‹è…• (Wrist, index 0) åœ¨å®‰å…¨åŒºåŸŸå†…
        """
        if not self.check_border:
            return True
        
        wrist = hand_landmarks.landmark[self.mp_hands.HandLandmark.WRIST]
        
        # æ£€æŸ¥ x, y æ˜¯å¦åœ¨ [0.05, 0.95] åŒºé—´
        if (self.margin < wrist.x < 1 - self.margin) and \
           (self.margin < wrist.y < 1 - self.margin):
            return True
        return False

    def process_video(self, input_path, output_path=None, visualize=False):
        print(f"ğŸ”„ Processing: {input_path}")
        cap = cv2.VideoCapture(input_path)
        if not cap.isOpened():
            return {"status": "Error", "reason": "Cannot open video"}

        # è§†é¢‘å‚æ•°
        fps = cap.get(cv2.CAP_PROP_FPS)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

        # ä»…å½“éœ€è¦å¯è§†åŒ–æ—¶æ‰åˆå§‹åŒ– Writer
        out = None
        if visualize and output_path:
            # Mac ç”¨æˆ·å¼ºçƒˆå»ºè®®ä½¿ç”¨ 'avc1' è€Œä¸æ˜¯ 'mp4v'
            # 'mp4v' åœ¨ Mac ä¸Šç»å¸¸å¯¼è‡´ç»¿å±æˆ–é©¬èµ›å…‹
            fourcc = cv2.VideoWriter_fourcc(*'avc1') 
            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
            mp_drawing = mp.solutions.drawing_utils

        # --- çŠ¶æ€è¿½è¸ªå™¨ ---
        frame_idx = 0
        consecutive_missing = 0  # å½“å‰è¿ç»­ä¸¢å¤±å¸§æ•°
        max_missing_streak = 0   # è®°å½•æ•´ä¸ªè§†é¢‘ä¸­æœ€ä¸¥é‡çš„è¿ç»­ä¸¢å¤±
        pass_frames = 0
        
        is_rejected = False
        reject_reason = ""

        start_time = time.time()

        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            frame_idx += 1

            # æ€§èƒ½ä¼˜åŒ–ï¼šåªåœ¨å¯è§†åŒ–å¼€å¯æ—¶æ‰åšæ·±æ‹·è´ï¼Œå¦åˆ™ç›´æ¥ç”¨åªè¯»å¼•ç”¨
            image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            image_rgb.flags.writeable = False 
            
            results = self.hands.process(image_rgb)
            
            # --- æ ¸å¿ƒåˆ¤å®šé€»è¾‘ ---
            valid_hands_count = 0
            if results.multi_hand_landmarks:
                for hand_lms in results.multi_hand_landmarks:
                    if self.is_hand_valid(hand_lms):
                        valid_hands_count += 1
            
            # åˆ¤å®šæ ‡å‡†ï¼šå‡è®¾è¦æ±‚å¿…é¡»åŒæ‰‹éƒ½åœ¨
            # å¦‚æœä½ çš„ä»»åŠ¡å…è®¸å•æ‰‹ï¼ŒæŠŠè¿™é‡Œæ”¹æˆ valid_hands_count >= 1
            is_frame_good = (valid_hands_count == 2)

            # --- ä¿®æ”¹åçš„ä»£ç  (Warm-up Masking) ---
            if is_frame_good:
                consecutive_missing = 0
                pass_frames += 1
            else:
                # åªæœ‰å½“è§†é¢‘æ’­æ”¾è¶…è¿‡ 60 å¸§ (çº¦ 2 ç§’) åï¼Œæ‰å¼€å§‹è®¡è¾ƒé”™è¯¯
                # è¿™æ ·å¯ä»¥è¿‡æ»¤æ‰æ¨¡å‹èµ·æ­¥æ—¶çš„"çŠ¹è±«"é˜¶æ®µ
                if frame_idx > 60:
                    consecutive_missing += 1
                    max_missing_streak = max(max_missing_streak, consecutive_missing)

            # ================= å…³é”®ä¿®æ”¹å¼€å§‹ =================
            # 1. MediaPipe ç”¨çš„æ˜¯ RGBï¼ŒOpenCV ç”»å›¾å’Œä¿å­˜è§†é¢‘éœ€è¦ BGR
            # 2. .copy() éå¸¸é‡è¦ï¼å®ƒèƒ½è§£å†³å†…å­˜ä¸è¿ç»­å¯¼è‡´çš„"é©¬èµ›å…‹/é›ªèŠ±"é—®é¢˜
            frame_bgr = None
            if visualize and out is not None:
                image_rgb.flags.writeable = True # ç¡®ä¿å¯å†™
                frame_bgr = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR).copy()
            # ================= å…³é”®ä¿®æ”¹ç»“æŸ =================

            # --- å¯è§†åŒ– (å¯é€‰) ---
            if visualize and out is not None:
                # æ³¨æ„ï¼šä»¥åæ‰€æœ‰çš„ç”»å›¾æ“ä½œ (cv2.rectangle, putText) 
                # éƒ½è¦ç”»åœ¨ ã€frame_bgrã€‘ ä¸Šï¼Œè€Œä¸æ˜¯åŸæ¥çš„ frame ä¸Šï¼
                
                # ç»˜åˆ¶æ‰‹éƒ¨
                if results.multi_hand_landmarks:
                    for hand_landmarks in results.multi_hand_landmarks:
                        # ç”»éª¨æ¶ (éœ€è¦æŠŠåŸæ¥çš„ frame æ¢æˆ frame_bgr)
                        self.mp_drawing.draw_landmarks(
                            frame_bgr, # <--- æ”¹è¿™é‡Œ
                            hand_landmarks,
                            self.mp_hands.HAND_CONNECTIONS
                        )
                        
                        # === ã€æ–°å¢ã€‘YOLO äº¤äº’æ£€æµ‹é€»è¾‘ ===
                        # 1. è·å–æ‰‹çš„æ¡†
                        h_box = self.get_hand_bbox(hand_landmarks, width, height)
                        
                        # 2. è°ƒç”¨ YOLO æ£€æŸ¥æ˜¯å¦æ‹¿ç€ä¸œè¥¿
                        # ä¼ ç»™ YOLO çš„å¯ä»¥ç”¨åŸæ¥çš„ frame (RGB) æˆ–è€… frame_bgr éƒ½å¯ä»¥ï¼ŒYOLOå¾ˆèªæ˜
                        # ä½†ç”»å›¾ä¸€å®šè¦ç”»åœ¨ frame_bgr ä¸Š
                        is_grasping_something, grasp_obj_name, o_box = self.check_interaction_with_yolo(frame_bgr, h_box)
                        
                        wrist_x = int(hand_landmarks.landmark[0].x * width)
                        wrist_y = int(hand_landmarks.landmark[0].y * height)
                        
                        # === ã€æ–°å¢ã€‘å‡ ä½•å§¿æ€è®¡ç®— (ç”¨æ¥è¿‡æ»¤"æ‚¬åœ"è¯¯åˆ¤) ===
                        # è®¡ç®—æ‰€æœ‰æŒ‡å°–åˆ°æ‰‹è…•çš„å¹³å‡è·ç¦»
                        # æ‰‹è…•(0), é£ŸæŒ‡(8), ä¸­æŒ‡(12), æ— åæŒ‡(16), å°æŒ‡(20)
                        wrist = np.array([hand_landmarks.landmark[0].x, hand_landmarks.landmark[0].y])
                        finger_tips = [
                            np.array([hand_landmarks.landmark[i].x, hand_landmarks.landmark[i].y])
                            for i in [8, 12, 16, 20]
                        ]
                        # è®¡ç®—å¹³å‡å¼ å¼€è·ç¦»
                        tips_to_wrist_dists = [np.linalg.norm(tip - wrist) for tip in finger_tips]
                        avg_spread = np.mean(tips_to_wrist_dists)
                        
                        # è®¾å®šä¸€ä¸ª"å¼ å¼€é˜ˆå€¼"
                        # > 0.40 è¯´æ˜æ‰‹æŒå®Œå…¨å¼ å¼€ï¼ˆåœ¨æ‚¬åœï¼‰
                        IS_HAND_OPEN = avg_spread > 0.40 
                        # ===============================================

                        # === æœ€ç»ˆåˆ¤å®šé€»è¾‘ (YOLO + Geometry) ===
                        final_state = "Free Hand"
                        color = (0, 255, 0) # Green

                        if is_grasping_something:
                            if IS_HAND_OPEN:
                                # 1. æ¡†é‡å äº†ï¼Œä½†æ˜¯æ‰‹æ˜¯å¼ å¼€çš„ -> æ‚¬åœ (Hovering)
                                final_state = f"Hovering: {grasp_obj_name}"
                                color = (255, 255, 0) # Cyan/Yellow (é»„è‰²è­¦å‘Š)
                            else:
                                # 2. æ¡†é‡å äº†ï¼Œä¸”æ‰‹æ˜¯å¼¯æ›²çš„ -> çœŸæŠ“å– (Grasping)
                                final_state = f"Grasping: {grasp_obj_name}"
                                color = (0, 0, 255) # Red
                            
                            # ç”»å‡ºç‰©ä½“çš„æ¡† (å¯é€‰ï¼Œä¸ºäº† Debug çœ‹å¾—æ›´æ¸…æ¥š)
                            if o_box:
                                cv2.rectangle(frame_bgr, (o_box[0], o_box[1]), (o_box[2], o_box[3]), color, 2)
                                cv2.putText(frame_bgr, grasp_obj_name, (o_box[0], o_box[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
                        
                        # === ç”»å›¾ ===
                        # ç”»æ‰‹éƒ¨æ¡†
                        cv2.rectangle(frame_bgr, (h_box[0], h_box[1]), (h_box[2], h_box[3]), color, 2)
                        
                        # å†™æ–‡å­—
                        cv2.putText(frame_bgr, final_state, (wrist_x, wrist_y - 20),
                                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
                        # ====================================
                
                # ç»˜åˆ¶çŠ¶æ€ä¿¡æ¯
                color = (0, 255, 0) if is_frame_good else (0, 0, 255)
                cv2.putText(frame_bgr, f"Hands: {valid_hands_count} | MissStreak: {consecutive_missing}", 
                            (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)
                
                # ç»˜åˆ¶å®‰å…¨æ¡† (Safe Zone)
                h, w, _ = frame_bgr.shape
                p1 = (int(w * self.margin), int(h * self.margin))
                p2 = (int(w * (1-self.margin)), int(h * (1-self.margin)))
                cv2.rectangle(frame_bgr, p1, p2, (255, 255, 0), 1)
                
                # ================= å†™å…¥è§†é¢‘ =================
                # ç¡®ä¿å†™å…¥çš„æ˜¯ frame_bgr
                out.write(frame_bgr)

            # --- Early Stopping (å¯é€‰ï¼šå¦‚æœåªæƒ³è¿‡æ»¤æ‰åæ•°æ®ï¼Œå‘ç°å¤ªå·®ç›´æ¥é€€å‡º) ---
            # if consecutive_missing > self.tolerance * 5: 
            #     is_rejected = True
            #     reject_reason = "Too many missing frames"
            #     break 

        # --- åœ¨ cap.release() ä¹‹å‰åŠ å…¥è¿™ä¸ªåˆ¤æ–­ ---
        if consecutive_missing > self.tolerance:
            loss_start_time = (frame_idx - consecutive_missing) / fps
            loss_end_time = frame_idx / fps
            print(f"âš ï¸ å‘ç°ç»“å°¾ä¸¢å¸§: {loss_start_time:.2f}ç§’ -> {loss_end_time:.2f}ç§’ (æŒç»­ {consecutive_missing} å¸§)")

        cap.release()
        if out: out.release()
        
        duration = time.time() - start_time
        fps_process = frame_idx / duration if duration > 0 else 0

        # --- æœ€ç»ˆåˆ¤å®š ---
        # è§„åˆ™ï¼šæœ€å¤§è¿ç»­ä¸¢å¤±ä¸èƒ½è¶…è¿‡ Tolerance (ä¾‹å¦‚ 10å¸§)
        if max_missing_streak > self.tolerance:
            final_status = "REJECT"
            reject_reason = f"Continuous missing frames exceeded limit ({max_missing_streak} > {self.tolerance})"
        else:
            final_status = "PASS"
            reject_reason = "None"

        return {
            "video": input_path,
            "status": final_status,
            "reason": reject_reason,
            "max_missing_streak": max_missing_streak,
            "pass_ratio": pass_frames / frame_idx if frame_idx > 0 else 0,
            "process_fps": f"{fps_process:.1f}",
            "duration_seconds": f"{duration:.2f}"
        }

# --- ä½¿ç”¨ç¤ºä¾‹ ---
if __name__ == "__main__":
    # --- åŸä»£ç  ---
    # filter_tool = HandDataFilter(min_conf=0.7, missing_tolerance=10, check_border=True)
    
    # --- ä¿®æ”¹åï¼šå°† min_conf æ”¹ä¸º 0.5 ---
    # 0.5 æ˜¯ MediaPipe å®˜æ–¹æ¨èçš„é»˜è®¤å€¼ï¼Œè¶³ä»¥è¿‡æ»¤æ‰æ˜æ˜¾çš„èƒŒæ™¯è¯¯æ£€ï¼Œä½†ä¸ä¼šè¯¯æ€çœŸæ‰‹
    filter_tool = HandDataFilter(min_conf=0.5, missing_tolerance=10, check_border=True)
    
    # æ¨¡å¼ A: å¿«é€Ÿè¿‡æ»¤ (ä¸ç”Ÿæˆè§†é¢‘ï¼Œé€Ÿåº¦å¿«)
    result = filter_tool.process_video("test_video.mp4", visualize=False)
    print("ğŸ“Š å¿«é€Ÿæ‰«æç»“æœ:", result)
    
    # æ¨¡å¼ B: Debug æ¨¡å¼ (ç”Ÿæˆè§†é¢‘ï¼ŒæŸ¥çœ‹å“ªé‡Œå‡ºäº†é—®é¢˜)
    if result["status"] == "REJECT":
        print("ğŸ” æ­£åœ¨ç”Ÿæˆ Debug è§†é¢‘...")
        filter_tool.process_video("test_video.mp4", "debug_output.mp4", visualize=True)

